ğŸ“˜ Knowledge Assistant â€“ RAG Powered Internal Query System

This is a simple RAG (Retrieval-Augmented Generation) assistant where users can:

Upload documents

Index them into a vector database

Ask any question

Receive context-aware answers using Gemini 2.5 Flash and local embeddings (all-MiniLM-L6-v2).

The system uses:

FastAPI backend

React + Vite frontend

FAISS for vector search

LangChain for text splitting + embedding pipeline

Gemini for generating final responses

ğŸ§  How It Works
1. Document Upload

Users upload .txt, .pdf, .md, or similar files.
They are saved to:

backend/data/raw_docs/

2. Vector Indexing

When the backend starts or when indexing is triggered, the system:

Loads raw documents

Splits them into chunks

Converts chunks into vector embeddings (all-MiniLM-L6-v2)

Stores them into a FAISS index at:

backend/data/vector_store/

3. Asking Questions

User enters a question in the frontend:

Question is sent to /chat

Backend embeds the question

Semantic search retrieves the most relevant chunks

Gemini uses those chunks as context

A final answer is generated and returned to the user

ğŸ§© Project Structure
knowledge_assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                # FastAPI main server
â”‚   â”œâ”€â”€ embeddings.py         # Embedding model and helper functions
â”‚   â”œâ”€â”€ retriever.py          # FAISS retriever + vector store logic
â”‚   â”œâ”€â”€ graph.py              # (Optional) LLM execution graph
â”‚   â”œâ”€â”€ requirements.txt      # Backend dependencies
â”‚   â”œâ”€â”€ .env                  # LLM API keys + model config
â”‚   â”‚
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ raw_docs/         # Uploaded documents
â”‚       â””â”€â”€ vector_store/     # FAISS vector index of embeddings
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx           # Main React component (UI)
    â”‚   â”œâ”€â”€ App.css           # Styling
    â”‚   â”œâ”€â”€ index.css
    â”‚   â”œâ”€â”€ main.jsx
    â”‚   â””â”€â”€ assets/
    â”œâ”€â”€ public/
    â”œâ”€â”€ package.json
    â””â”€â”€ vite.config.js

âš™ï¸ Backend Setup
1. Go to backend folder
cd backend

2. Create .env file
GOOGLE_API_KEY=your_key_here
MODEL_NAME=gemini-2.5-flash
EMB_MODEL=all-MiniLM-L6-v2

3. Install dependencies
pip install -r requirements.txt

4. Start backend
uvicorn app:app --reload --port 8000


Backend will run at:

http://localhost:8000

ğŸ’» Frontend Setup
cd frontend
npm install
npm run dev


Frontend runs at:

http://localhost:5173

ğŸ”‘ API Endpoints
1. Upload Document
POST /upload


Uploads a file and stores it in raw_docs/.

2. Rebuild Vector Index
POST /reindex


Processes all documents and updates FAISS embeddings.

3. Ask Question (RAG Chat)
POST /chat


Request body:

{
  "message": "Your question here"
}


Response:

{
  "answer": "AI-generated answer with context"
}
